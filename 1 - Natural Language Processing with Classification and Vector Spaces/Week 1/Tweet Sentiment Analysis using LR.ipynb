{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis usingLogistic Regression\n",
    "We will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative one. Specifically we will: \n",
    "\n",
    "* Learn how to extract features for logistic regression given some text\n",
    "* Implement logistic regression\n",
    "* Apply logistic regression on a natural language processing task\n",
    "* Test using logistic regression\n",
    "* Perform error analysis\n",
    "\n",
    "We will be using a data set of tweets. Hopefully we will get more than 99% accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.downloader.Downloader.download_dir = \"Your_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads sample twitter dataset. uncomment the line below if running on a local machine.\n",
    "# nltk.download('twitter_samples', \"C:/Users/pulki/OneDrive/Documents/Jupyter/NLP - Deeplearning.ai/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the path of downloaded data to nltk\n",
    "nltk.data.path.append(\"Your_path/nltk_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
    "    * If we used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * We will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine positive and negative labels\n",
    "tweets = all_positive_tweets+all_negative_tweets\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.ones(len(all_positive_tweets)), np.zeros(len(all_negative_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "x_train = all_positive_tweets[1000:] + all_negative_tweets[:-1000]\n",
    "y_train = labels[1000:9000]\n",
    "x_test = all_positive_tweets[:1000] + all_negative_tweets[-1000:]\n",
    "y_test = np.append(labels[:1000], labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweet\n",
    "Define a function `process_tweet()` to tokenize the tweet into individual words, remove stop words and apply stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?./\\/\\.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#*', '', tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet.lower())\n",
    "    \n",
    "    clean_tokens = [x for x in tokens if (x not in stopwords_english) & (x not in punctuation)]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in clean_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freq(tweet_list, labels):\n",
    "    wl_pair = []\n",
    "    for tweet, label in zip(tweet_list, labels):\n",
    "        for word in tweet:\n",
    "            wl_pair.append([word, label])\n",
    "    \n",
    "    freq = pd.DataFrame(wl_pair, columns=['vocab', 'label'])\n",
    "    \n",
    "    word_freq = pd.DataFrame(freq.groupby('vocab').sum())\n",
    "    \n",
    "    freq.label = 1-freq.label\n",
    "    word_freq['neg'] = freq.groupby('vocab').sum()\n",
    "    \n",
    "    return word_freq.convert_dtypes().rename(columns={'label': 'pos'}).reset_index()#.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-:</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(:</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>):</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>);</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vocab  pos  neg\n",
       "0   (-:    2    0\n",
       "1    (:    0    6\n",
       "2    ):    7    6\n",
       "3    );    1    0\n",
       "4  --->    1    0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function below\n",
    "freq_df = build_freq([preprocess_tweet(tweet) for tweet in x_train], y_train)\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "\n",
    "x = id(freq_df)\n",
    "y = ctypes.cast(x, ctypes.py_object).value\n",
    "\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extracting the features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. We will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train the logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tokenized_tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    return [1] + list(freq_df.loc[freq_df.vocab.isin(tokenized_tweet), ['pos', 'neg']].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2971, 3], [1, 565, 94], [1, 3910, 356], [1, 3849, 165], [1, 625, 5]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the function on test set\n",
    "train_features = [extract_features(preprocess_tweet(tweet)) for tweet in x_train]\n",
    "train_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a Logistic Regression Model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @eawoman As a Hull supporter I am expecting a misserable few weeks :-(. \n",
      "Prediction: [0.]\n"
     ]
    }
   ],
   "source": [
    "# Test with a tweet\n",
    "print(f'Tweet: {x_test[-1]}. \\nPrediction: {lr.predict([extract_features(preprocess_tweet(x_test[-1]))])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Logistic regression \n",
    "\n",
    "\n",
    "### Part 1.1: Sigmoid\n",
    "We will use logistic regression for text classification. \n",
    "* The sigmoid function is defined as: \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". \n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update the weight vector $\\theta$, we will apply gradient descent to iteratively improve our model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Test logistic regression\n",
    "\n",
    "It is time to test the logistic regression function on some new input that the model has not seen before. \n",
    "\n",
    "#### Instructions: Write `predict_tweet`\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.37774253e-12, 1.00000000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba([extract_features(preprocess_tweet(x_test[2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, model):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        model: trained model\n",
    "    Output: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    return model.predict_proba([extract_features(preprocess_tweet(tweet))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27177522,  0.00873689, -0.01001496]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> [0.12609435 0.87390565] -> [1.]\n",
      "I am bad -> [0.46659585 0.53340415] -> [1.]\n",
      "this movie should have been great. -> [0.1687119 0.8312881] -> [1.]\n",
      "great -> [0.17057416 0.82942584] -> [1.]\n",
      "great great -> [0.17057416 0.82942584] -> [1.]\n",
      "great great great -> [0.17057416 0.82942584] -> [1.]\n",
      "great great great great -> [0.17057416 0.82942584] -> [1.]\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( f'{tweet} -> {predict_tweet(tweet, lr)[0]} -> {lr.predict([extract_features(preprocess_tweet(tweet))])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.40039108e-12, 1.00000000e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, lr)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance using the test set\n",
    "After training our model using the training set above, check how our model might perform on real, unseen data, by testing it against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, model):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        model: Trained Logistic Regression Model\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    y_pred = model.predict([extract_features(preprocess_tweet(tweet)) for tweet in test_x])\n",
    "    \n",
    "    print('Confusion Matrix: \\n', confusion_matrix(test_y, y_pred))\n",
    "    print(classification_report(test_y, y_pred))\n",
    "    \n",
    "    return sum([x==y for x,y in zip(y_pred, test_y)]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[993   7]\n",
      " [  6 994]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      1000\n",
      "         1.0       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9935"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print accuracy\n",
    "test_logistic_regression(x_test, y_test, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Error Analysis\n",
    "\n",
    "In this part we will see some tweets that the model misclassified and to try to inspect why did the misclassifications happened and specifically what kind of tweets does the model misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/cHl12JvuxN http://t.co/GGgU9PYEjI\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot', 't.co/chl12jvuxn', 't.co/gggu9pyeji']\n",
      "1\t0.36376919\tb\"i'm play brain dot braindot t.co/chl12jvuxn t.co/gggu9pyeji\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/MifDDs7CQS http://t.co/WtIWoeATPj\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot', 't.co/mifdds7cq', 't.co/wtiwoeatpj']\n",
      "1\t0.36376919\tb\"i'm play brain dot braindot t.co/mifdds7cq t.co/wtiwoeatpj\"\n",
      "THE TWEET IS: I still fully intend to write as many game designs as possible while there. And an attack plan for the next 6 months. &gt;:D\n",
      "THE PROCESSED TWEET IS: ['still', 'fulli', 'intend', 'write', 'mani', 'game', 'design', 'possibl', 'attack', 'plan', 'next', '6', 'month', '>:d']\n",
      "1\t0.47590703\tb'still fulli intend write mani game design possibl attack plan next 6 month >:d'\n",
      "THE TWEET IS: Remember that one time I didn't go to flume/kaytranada/alunageorge even though I had tickets? I still want to kms. : ) : )\n",
      "THE PROCESSED TWEET IS: ['rememb', 'one', 'time', 'go', 'flume', 'kaytranada', 'alunageorg', 'even', 'though', 'ticket', 'still', 'want', 'km']\n",
      "1\t0.03784486\tb'rememb one time go flume kaytranada alunageorg even though ticket still want km'\n",
      "THE TWEET IS: We are thrilled to be on our first international assignment as aerial camera operators....Wales here we come #Boom : ) Never forget to smile\n",
      "THE PROCESSED TWEET IS: ['thrill', 'first', 'intern', 'assign', 'aerial', 'camera', 'oper', '...', 'wale', 'come', 'boom', 'never', 'forget', 'smile']\n",
      "1\t0.47348744\tb'thrill first intern assign aerial camera oper ... wale come boom never forget smile'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/ilDzDRHf9d http://t.co/VTXNFCPFuI\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot', 't.co/ildzdrhf9d', 't.co/vtxnfcpfui']\n",
      "1\t0.36376919\tb\"i'm play brain dot braindot t.co/ildzdrhf9d t.co/vtxnfcpfui\"\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['@phenomyoutub', 'u', 'prob', 'fun', 'david']\n",
      "0\t0.65940569\tb'@phenomyoutub u prob fun david'\n",
      "THE TWEET IS: @bumkeyyfel b-butt : ( isn't black cat a bad luck ene\n",
      "THE PROCESSED TWEET IS: ['@bumkeyyfel', 'b-butt', 'black', 'cat', 'bad', 'luck', 'ene']\n",
      "0\t0.52167064\tb'@bumkeyyfel b-butt black cat bad luck ene'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.63871271\tb'pat jay'\n",
      "THE TWEET IS: @bae_ts WHATEVER STIL L YOUNG &gt;:-(\n",
      "THE PROCESSED TWEET IS: ['@bae_t', 'whatev', 'stil', 'l', 'young', '>:-(']\n",
      "0\t0.63432577\tb'@bae_t whatev stil l young >:-('\n",
      "THE TWEET IS: the internet is being a total bitch : (\n",
      "THE PROCESSED TWEET IS: ['internet', 'total', 'bitch']\n",
      "0\t0.59431765\tb'internet total bitch'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth', 't.co/wt4oxq5xcf']\n",
      "0\t0.63264264\tb'belov grandmoth t.co/wt4oxq5xcf'\n",
      "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
      "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 't.co/ktknmhvwci', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
      "0\t0.67049626\tb'sr financi analyst expedia inc bellevu wa t.co/ktknmhvwci financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(x_test, y_test):\n",
    "    y_hat = predict_tweet(x, lr)[0,1]\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', preprocess_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(preprocess_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Predict with new tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "0.3171086659548087\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(preprocess_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, lr)[0,1]\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
