{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes\n",
    "Welcome to week two of this specialization. You will learn about Naive Bayes. Concretely, you will be using Naive Bayes for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
    "\n",
    "* Train a naive bayes model on a sentiment analysis task\n",
    "* Test using your model\n",
    "* Compute ratios of positive words to negative words\n",
    "* Do some error analysis\n",
    "* Predict on your own tweet\n",
    "\n",
    "You may already be familiar with Naive Bayes and its justification in terms of conditional probabilities and independence.\n",
    "* In this week's lectures and assignments we used the ratio of probabilities between positive and negative sentiments.\n",
    "* This approach gives us simpler formulas for these 2-way classification tasks.\n",
    "\n",
    "Load the cell below to import some packages.\n",
    "You  may want to browse the documentation of unfamiliar libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pdb\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from my_nlp_utils import preprocess_tweet, build_freq, extract_features, predict_tweet, test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook in your local computer,\n",
    "don't forget to download the twitter samples and stopwords from nltk.\n",
    "\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'C:/Users/pulki/OneDrive/Documents/Jupyter/NLP - Deeplearning.ai/nltk_data' not in nltk.data.path:\n",
    "    # add path from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "    nltk.data.path.append('C:/Users/pulki/OneDrive/Documents/Jupyter/NLP - Deeplearning.ai/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# combine positive and negative labels\n",
    "tweets = all_positive_tweets+all_negative_tweets\n",
    "\n",
    "# Create a labels array\n",
    "labels = np.append(np.ones(len(all_positive_tweets)), np.zeros(len(all_negative_tweets)))\n",
    "\n",
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "x_train = all_positive_tweets[1000:] + all_negative_tweets[:-1000]\n",
    "y_train = labels[1000:9000]\n",
    "x_test = all_positive_tweets[:1000] + all_negative_tweets[-1000:]\n",
    "y_test = np.append(labels[:1000], labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Process the Data\n",
    "\n",
    "For any machine learning project, once you've gathered the data, the first step is to process it to make useful inputs to your model.\n",
    "- **Remove noise**: You will first want to remove noise from your data -- that is, remove words that don't tell you much about the content. These include all common words like 'I, you, are, is, etc...' that would not give us enough information on the sentiment.\n",
    "- We'll also remove stock market tickers, retweet symbols, hyperlinks, and hashtags because they can not tell you a lot of information on the sentiment.\n",
    "- You also want to remove all the punctuation from a tweet. The reason for doing this is because we want to treat words with or without the punctuation as the same word, instead of treating \"happy\", \"happy?\", \"happy!\", \"happy,\" and \"happy.\" as different words.\n",
    "- Finally you want to use stemming to only keep track of one variation of each word. In other words, we'll treat \"motivation\", \"motivated\", and \"motivate\" similarly by grouping them within the same stem of \"motiv-\".\n",
    "\n",
    "We have given you the function `process_tweet()` that does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', ':)', 'good', 'morn', 'httpchapagain.com.np']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "# print cleaned tweet\n",
    "print(preprocess_tweet(custom_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Implementing your helper functions\n",
    "\n",
    "To help train your naive bayes model, lets build a table where the keys are a words and the values are the corresponding positive and negative frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-:</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(:</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>):</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>);</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vocab  pos  neg\n",
       "0   (-:    2    0\n",
       "1    (:    0    6\n",
       "2    ):    7    6\n",
       "3    );    1    0\n",
       "4  --->    1    0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function below\n",
    "freq_df = build_freq([preprocess_tweet(tweet) for tweet in x_train], y_train)\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.99921088992485"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.log(freq_df.pos/freq_df.pos.sum()+1)*1000).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-:</td>\n",
       "      <td>0.068868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(:</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>):</td>\n",
       "      <td>0.241018</td>\n",
       "      <td>0.216474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>);</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---&gt;</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vocab       pos       neg\n",
       "0   (-:  0.068868       0.0\n",
       "1    (:       0.0  0.216474\n",
       "2    ):  0.241018  0.216474\n",
       "3    );  0.034435       0.0\n",
       "4  --->  0.034435       0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.pos = np.log(freq_df.pos/freq_df.pos.sum()+1.0)*1000\n",
    "freq_df.neg = np.log(freq_df.neg/freq_df.neg.sum()+1.0)*1000\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 97.37794838278371, 0.10824528385622484],\n",
       " [1, 19.259081232765254, 3.3892461594286543],\n",
       " [1, 129.5604900299557, 12.82805422340109],\n",
       " [1, 127.28114515739703, 5.947078757620799],\n",
       " [1, 21.333716268304208, 0.18039795838442155]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the function on test set\n",
    "train_features = [extract_features(preprocess_tweet(tweet), freq_df) for tweet in x_train]\n",
    "train_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a Naive Bayes Model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!. \n",
      "Prediction: [1.]\n"
     ]
    }
   ],
   "source": [
    "# Test with a tweet\n",
    "print(f'Tweet: {x_test[1]}. \\nPrediction: {gnb.predict([extract_features(preprocess_tweet(x_test[1]), freq_df)])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Train your model using Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### So how do you train a Naive Bayes classifier?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_tweets()` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared.\n",
    "\n",
    "We will use this dictionary in several parts of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
    "\n",
    "##### Calculate $V$\n",
    "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
    "\n",
    "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
    "\n",
    "##### Calculate $N_{pos}$, $N_{neg}$, $V_{pos}$, and $V_{neg}$\n",
    "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
    "- Similarly, use `freqs` dictionary to compute the total number of **unique** positive words, $V_{pos}$, and total **unique** negative words $V_{neg}$.\n",
    "\n",
    "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
    "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
    "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
    "\n",
    "##### Calculate the logprior\n",
    "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "0.0\n",
    "\n",
    "9089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Test your naive bayes\n",
    "\n",
    "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
    "\n",
    "#### Implement `naive_bayes_predict`\n",
    "**Instructions**:\n",
    "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
    "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Note\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my_nlp_utils import predict_tweet\n",
    "# import sys\n",
    "# del sys.modules['my_nlp_utils']\n",
    "# from importlib import reload\n",
    "# reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is [1.]\n"
     ]
    }
   ],
   "source": [
    "# Experiment with your own tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = predict_tweet(my_tweet, gnb, freq_df)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.86055203, -0.05894052]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.predict_log_proba([extract_features(preprocess_tweet(my_tweet), freq_df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- The expected output is around 1.57\n",
    "- The sentiment is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement test_naive_bayes\n",
    "**Instructions**:\n",
    "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
    "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
    "* It returns the accuracy of your model.\n",
    "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[932  68]\n",
      " [  1 999]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96      1000\n",
      "         1.0       0.94      1.00      0.97      1000\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "Naive Bayes accuracy = 0.9655\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_model(x_test, y_test, gnb, freq_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Accuracy**:\n",
    "\n",
    "0.9940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> [1.]\n",
      "I am bad -> [1.]\n",
      "this movie should have been great. -> [1.]\n",
      "great -> [1.]\n",
      "great great -> [1.]\n",
      "great great great -> [1.]\n",
      "great great great great -> [1.]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    p = predict_tweet(tweet, gnb, freq_df)\n",
    "    print(f'{tweet} -> {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "- I am happy -> 2.15\n",
    "- I am bad -> -1.29\n",
    "- this movie should have been great. -> 2.14\n",
    "- great -> 2.14\n",
    "- great great -> 4.28\n",
    "- great great great -> 6.41\n",
    "- great great great great -> 8.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'you are bad :('\n",
    "predict_tweet(my_tweet, gnb, freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Filter words by Ratio of positive to negative counts\n",
    "\n",
    "- Some words have more positive counts than others, and can be considered \"more positive\".  Likewise, some words can be considered more negative than others.\n",
    "- One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n",
    "    - Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.\n",
    "- We can calculate the ratio of positive to negative frequencies of a word.\n",
    "- Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n",
    "- Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n",
    "\n",
    "#### Implement `get_ratio()`\n",
    "- Given the `freqs` dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.\n",
    "- Similarly, use the `lookup()` function to get the negative count of that word.\n",
    "- Calculate the ratio of positive divided by negative counts\n",
    "\n",
    "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
    "\n",
    "Where pos_words and neg_words correspond to the frequency of the words in their respective classes. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Words</b>\n",
    "        </td>\n",
    "        <td>\n",
    "        Positive word count\n",
    "        </td>\n",
    "         <td>\n",
    "        Negative Word Count\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        glad\n",
    "        </td>\n",
    "         <td>\n",
    "        41\n",
    "        </td>\n",
    "    <td>\n",
    "        2\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        arriv\n",
    "        </td>\n",
    "         <td>\n",
    "        57\n",
    "        </td>\n",
    "    <td>\n",
    "        4\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :(\n",
    "        </td>\n",
    "         <td>\n",
    "        1\n",
    "        </td>\n",
    "    <td>\n",
    "        3663\n",
    "        </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        :-(\n",
    "        </td>\n",
    "         <td>\n",
    "        0\n",
    "        </td>\n",
    "    <td>\n",
    "        378\n",
    "        </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(word, df):\n",
    "    '''\n",
    "    Input:\n",
    "        df: a dataframe mapping each word to its positive and negative sentiment counts\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    ratio = df.loc[df.vocab == word, ['pos', 'neg']].to_dict(orient='list')\n",
    "    \n",
    "    ratio['pos'] = ratio['pos'][0]\n",
    "    ratio['neg'] = ratio['neg'][0]\n",
    "    ratio['ratio'] = ratio['pos']/ratio['neg']\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 6.179216365958118, 'neg': 0.6492804037206948, 'ratio': 9.517022738632155}\n"
     ]
    }
   ],
   "source": [
    "print(get_ratio('happi', freq_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement `get_words_by_threshold(freqs,label,threshold)`\n",
    "\n",
    "* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n",
    "* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n",
    "* Use the `get_ratio()` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n",
    "* Append a dictionary to a list, where the key is the word, and the dictionary is the dictionary `pos_neg_ratio` that is returned by the `get_ratio()` function.\n",
    "An example key-value pair would have this structure:\n",
    "```\n",
    "{'happi':\n",
    "    {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def get_words_by_threshold(label, threshold, df):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary of words\n",
    "        pos_neg_ratio: dictionary of positive counts, negative counts, and ratio of positive / negative counts.\n",
    "        label: 1 for positive, 0 for negative\n",
    "        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n",
    "    Output:\n",
    "        word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.\n",
    "        example of a key value pair:\n",
    "        {'happi':\n",
    "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "        }\n",
    "    '''\n",
    "    if label:\n",
    "        return df[(df.pos+1)/(df.neg+1) > threshold] \n",
    "    else:\n",
    "        return df[(df.pos+1)/(df.neg+1) < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>:(</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>124.583527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vocab       pos         neg\n",
       "441    :(  0.034435  124.583527"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function: find negative words at or below a threshold 0.05\n",
    "get_words_by_threshold(0, 0.05, freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(-:</td>\n",
       "      <td>0.068868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>);</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>---&gt;</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>--&gt;</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-&gt;</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10870</th>\n",
       "      <td>🙆</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>🙌</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>🚮</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10876</th>\n",
       "      <td>🚲</td>\n",
       "      <td>0.068868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10878</th>\n",
       "      <td>󾰀</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4666 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      vocab       pos  neg\n",
       "0       (-:  0.068868  0.0\n",
       "3        );  0.034435  0.0\n",
       "4      --->  0.034435  0.0\n",
       "5       -->  0.034435  0.0\n",
       "6        ->  0.034435  0.0\n",
       "...     ...       ...  ...\n",
       "10870     🙆  0.034435  0.0\n",
       "10871     🙌  0.034435  0.0\n",
       "10875     🚮  0.034435  0.0\n",
       "10876     🚲  0.068868  0.0\n",
       "10878     󾰀  0.034435  0.0\n",
       "\n",
       "[4666 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function; find positive words at or above a threshold 10\n",
    "get_words_by_threshold(1, 10, freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the positive and negative ratios. Emojis like :( and words like 'me' tend to have a negative connotation. Other words like 'glad', 'community', and 'arrives' tend to be found in the positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model missclassified. Why do you think the misclassifications happened? Were there any assumptions made by the naive bayes model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: Remember that one time I didn't go to flume/kaytranada/alunageorge even though I had tickets? I still want to kms. : ) : )\n",
      "THE PROCESSED TWEET IS: ['rememb', 'one', 'time', 'go', 'flume', 'kaytranada', 'alunageorg', 'even', 'though', 'ticket', 'still', 'want', 'km']\n",
      "1\t0.00\tb'rememb one time go flume kaytranada alunageorg even though ticket still want km'\n",
      "THE TWEET IS: It's really hot :-(\n",
      "THE PROCESSED TWEET IS: ['realli', 'hot', ':-(']\n",
      "0\t1.00\tb'realli hot :-('\n",
      "THE TWEET IS: @Mjbulanhagui13 agh, sorry :-(\n",
      "THE PROCESSED TWEET IS: ['agh', 'sorri', ':-(']\n",
      "0\t1.00\tb'agh sorri :-('\n",
      "THE TWEET IS: Alone :-( :'( :-\\\n",
      "THE PROCESSED TWEET IS: ['alon', ':-(', \":'(\", ':-\\\\']\n",
      "0\t1.00\tb\"alon :-( :'( :-\\\\\"\n",
      "THE TWEET IS: @baexrv pcy mine :-(\n",
      "THE PROCESSED TWEET IS: ['pci', 'mine', ':-(']\n",
      "0\t1.00\tb'pci mine :-('\n",
      "THE TWEET IS: Get in the bin, OSX/Chrome/Voiceover &gt;:( http://t.co/0bcvA6YjWu\n",
      "THE PROCESSED TWEET IS: ['get', 'bin', 'osx', 'chrome', 'voiceov', '>:(', 'httpt.co/0bcva6yjwu']\n",
      "0\t1.00\tb'get bin osx chrome voiceov >:( httpt.co/0bcva6yjwu'\n",
      "THE TWEET IS: @norman__g lucky spike only :-(\n",
      "THE PROCESSED TWEET IS: ['lucki', 'spike', ':-(']\n",
      "0\t1.00\tb'lucki spike :-('\n",
      "THE TWEET IS: @Lizarrdz That's no excuse for launching an attack Lizardz, you should feel like shit for doing that.  I am deeply ashamed of you.  &gt;:(\n",
      "THE PROCESSED TWEET IS: [\"that'\", 'excus', 'launch', 'attack', 'lizardz', 'feel', 'like', 'shit', 'deepli', 'asham', '>:(']\n",
      "0\t1.00\tb\"that' excus launch attack lizardz feel like shit deepli asham >:(\"\n",
      "THE TWEET IS: Yeah, leave me on read :-(\n",
      "THE PROCESSED TWEET IS: ['yeah', 'leav', 'read', ':-(']\n",
      "0\t1.00\tb'yeah leav read :-('\n",
      "THE TWEET IS: @Marguuuuh it's raining :-(\n",
      "THE PROCESSED TWEET IS: ['rain', ':-(']\n",
      "0\t1.00\tb'rain :-('\n",
      "THE TWEET IS: @DasCarrot no EZOO for me this year sorry :-(\n",
      "THE PROCESSED TWEET IS: ['ezoo', 'year', 'sorri', ':-(']\n",
      "0\t1.00\tb'ezoo year sorri :-('\n",
      "THE TWEET IS: My neighbor doesn't let me sleep :-(\n",
      "THE PROCESSED TWEET IS: ['neighbor', 'let', 'sleep', ':-(']\n",
      "0\t1.00\tb'neighbor let sleep :-('\n",
      "THE TWEET IS: Drop Dead Fred use to be my favorite movie \n",
      "I wish I had it :-(\n",
      "THE PROCESSED TWEET IS: ['drop', 'dead', 'fred', 'use', 'favorit', 'movi', 'wish', ':-(']\n",
      "0\t1.00\tb'drop dead fred use favorit movi wish :-('\n",
      "THE TWEET IS: @fivedorkz why omg :-(\n",
      "THE PROCESSED TWEET IS: ['omg', ':-(']\n",
      "0\t1.00\tb'omg :-('\n",
      "THE TWEET IS: @nighteyeslrh and idk what to do bc we have no money if we leave :-(\n",
      "THE PROCESSED TWEET IS: ['idk', 'bc', 'money', 'leav', ':-(']\n",
      "0\t1.00\tb'idk bc money leav :-('\n",
      "THE TWEET IS: goodbye stage already :-(\n",
      "THE PROCESSED TWEET IS: ['goodby', 'stage', 'alreadi', ':-(']\n",
      "0\t1.00\tb'goodby stage alreadi :-('\n",
      "THE TWEET IS: today was tiring :-((\n",
      "THE PROCESSED TWEET IS: ['today', 'tire', ':-(']\n",
      "0\t1.00\tb'today tire :-('\n",
      "THE TWEET IS: My mum says she's done &amp; that she's leaving my step dad :-(\n",
      "THE PROCESSED TWEET IS: ['mum', 'say', 'done', 'leav', 'step', 'dad', ':-(']\n",
      "0\t1.00\tb'mum say done leav step dad :-('\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t1.00\tb'u prob fun david'\n",
      "THE TWEET IS: @cmoan3 don't say that :-(\n",
      "THE PROCESSED TWEET IS: ['say', ':-(']\n",
      "0\t1.00\tb'say :-('\n",
      "THE TWEET IS: @kennyfairley @kevrobbo27 i don't think we will ever win the Petrofac Cup :-( ;-)\n",
      "THE PROCESSED TWEET IS: ['think', 'ever', 'win', 'petrofac', 'cup', ':-(', ';-)']\n",
      "0\t1.00\tb'think ever win petrofac cup :-( ;-)'\n",
      "THE TWEET IS: the weather is set for more sleep but responsibilities. :-(\n",
      "THE PROCESSED TWEET IS: ['weather', 'set', 'sleep', 'respons', ':-(']\n",
      "0\t1.00\tb'weather set sleep respons :-('\n",
      "THE TWEET IS: Guys nooooo :-( what a WOW!!!!! Wow wow wow\n",
      "THE PROCESSED TWEET IS: ['guy', 'nooooo', ':-(', 'wow', 'wow', 'wow', 'wow']\n",
      "0\t1.00\tb'guy nooooo :-( wow wow wow wow'\n",
      "THE TWEET IS: @bumkeyyfel b-butt : ( isn't black cat a bad luck ene\n",
      "THE PROCESSED TWEET IS: ['b-butt', 'black', 'cat', 'bad', 'luck', 'ene']\n",
      "0\t1.00\tb'b-butt black cat bad luck ene'\n",
      "THE TWEET IS: @kaydeejimin okies :-(\n",
      "THE PROCESSED TWEET IS: ['oki', ':-(']\n",
      "0\t1.00\tb'oki :-('\n",
      "THE TWEET IS: omg when ally hugs mani she wraps her arms around her neck and pulls her closer :-(((\n",
      "THE PROCESSED TWEET IS: ['omg', 'alli', 'hug', 'mani', 'wrap', 'arm', 'around', 'neck', 'pull', 'closer', ':-(']\n",
      "0\t1.00\tb'omg alli hug mani wrap arm around neck pull closer :-('\n",
      "THE TWEET IS: @SensodyneIndia I loved it but this #ToothSensitivity :-(\n",
      "THE PROCESSED TWEET IS: ['love', 'toothsensit', ':-(']\n",
      "0\t1.00\tb'love toothsensit :-('\n",
      "THE TWEET IS: @ashtonboyf IMS OSAD :-(\n",
      "THE PROCESSED TWEET IS: ['im', 'osad', ':-(']\n",
      "0\t1.00\tb'im osad :-('\n",
      "THE TWEET IS: @ZappingZach same :-(\n",
      "THE PROCESSED TWEET IS: [':-(']\n",
      "0\t1.00\tb':-('\n",
      "THE TWEET IS: @deefizzy :-( poor boy\n",
      "THE PROCESSED TWEET IS: [':-(', 'poor', 'boy']\n",
      "0\t1.00\tb':-( poor boy'\n",
      "THE TWEET IS: why isn't anyone awake :-((\n",
      "THE PROCESSED TWEET IS: ['anyon', 'awak', ':-(']\n",
      "0\t1.00\tb'anyon awak :-('\n",
      "THE TWEET IS: @MonsterxStitch ya sorry :-(\n",
      "THE PROCESSED TWEET IS: ['ya', 'sorri', ':-(']\n",
      "0\t1.00\tb'ya sorri :-('\n",
      "THE TWEET IS: @FreedomChild3 Had wondered why this has not happened earlier? But then I realized, we don't have the leadership to do it!  :-( #wakeupGOP\n",
      "THE PROCESSED TWEET IS: ['wonder', 'happen', 'earlier', 'realiz', 'leadership', ':-(', 'wakeupgop']\n",
      "0\t1.00\tb'wonder happen earlier realiz leadership :-( wakeupgop'\n",
      "THE TWEET IS: @carterreynolds @hankgreen because you're asian!!!! ummmm or maybe because ur filmed child pornography, slutshamed, faked suicide,ect. :-(\n",
      "THE PROCESSED TWEET IS: ['asian', 'ummmm', 'mayb', 'ur', 'film', 'child', 'pornographi', 'slutsham', 'fake', 'suicid', 'ect', ':-(']\n",
      "0\t1.00\tb'asian ummmm mayb ur film child pornographi slutsham fake suicid ect :-('\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t1.00\tb'pat jay'\n",
      "THE TWEET IS: there's nothing to do :-(\n",
      "THE PROCESSED TWEET IS: [\"there'\", 'noth', ':-(']\n",
      "0\t1.00\tb\"there' noth :-(\"\n",
      "THE TWEET IS: !! NUGGETS AND FRIES !! :-((\n",
      "THE PROCESSED TWEET IS: ['nugget', 'fri', ':-(']\n",
      "0\t1.00\tb'nugget fri :-('\n",
      "THE TWEET IS: Love Warrior :-(\n",
      "THE PROCESSED TWEET IS: ['love', 'warrior', ':-(']\n",
      "0\t1.00\tb'love warrior :-('\n",
      "THE TWEET IS: @bae_ts WHATEVER STIL L YOUNG &gt;:-(\n",
      "THE PROCESSED TWEET IS: ['whatev', 'stil', 'l', 'young', '>:-(']\n",
      "0\t1.00\tb'whatev stil l young >:-('\n",
      "THE TWEET IS: don't sleep. I'm here : (\n",
      "THE PROCESSED TWEET IS: ['sleep', \"i'm\"]\n",
      "0\t1.00\tb\"sleep i'm\"\n",
      "THE TWEET IS: Bad day???:-(\n",
      "THE PROCESSED TWEET IS: ['bad', 'day', ':-(']\n",
      "0\t1.00\tb'bad day :-('\n",
      "THE TWEET IS: @marlenejazmyne idk :-( maybe because I think boobs are more fun to play with\n",
      "THE PROCESSED TWEET IS: ['idk', ':-(', 'mayb', 'think', 'boob', 'fun', 'play']\n",
      "0\t1.00\tb'idk :-( mayb think boob fun play'\n",
      "THE TWEET IS: @GZTAEHUN ok :-( i only have my d &amp; w hahaahahahaha\n",
      "THE PROCESSED TWEET IS: ['ok', ':-(', 'w', 'hahaahahahaha']\n",
      "0\t1.00\tb'ok :-( w hahaahahahaha'\n",
      "THE TWEET IS: i got so excited when micha rt or fave my tweet why am i such a creep :-( @japhantrash\n",
      "THE PROCESSED TWEET IS: ['got', 'excit', 'micha', 'rt', 'fave', 'tweet', 'creep', ':-(', '@japhantrash']\n",
      "0\t1.00\tb'got excit micha rt fave tweet creep :-( @japhantrash'\n",
      "THE TWEET IS: @rebeccalowrie No, it's not just you.  Twitter have decided to take that feature away :-(\n",
      "THE PROCESSED TWEET IS: ['twitter', 'decid', 'take', 'featur', 'away', ':-(']\n",
      "0\t1.00\tb'twitter decid take featur away :-('\n",
      "THE TWEET IS: @CNET Thats why they been pushing emoji, I give Sony five  :-(  :-(  :-(  :-( :-(  :-(\n",
      "THE PROCESSED TWEET IS: ['that', 'push', 'emoji', 'give', 'soni', 'five', ':-(', ':-(', ':-(', ':-(', ':-(', ':-(']\n",
      "0\t1.00\tb'that push emoji give soni five :-( :-( :-( :-( :-( :-('\n",
      "THE TWEET IS: This is it :-(\n",
      "THE PROCESSED TWEET IS: [':-(']\n",
      "0\t1.00\tb':-('\n",
      "THE TWEET IS: MTAP tomorrow which means I have to sleep early tonight. :-(\n",
      "THE PROCESSED TWEET IS: ['mtap', 'tomorrow', 'mean', 'sleep', 'earli', 'tonight', ':-(']\n",
      "0\t1.00\tb'mtap tomorrow mean sleep earli tonight :-('\n",
      "THE TWEET IS: @Velocentric Slow news day! :-(\n",
      "THE PROCESSED TWEET IS: ['slow', 'news', 'day', ':-(']\n",
      "0\t1.00\tb'slow news day :-('\n",
      "THE TWEET IS: Starving :-(\n",
      "THE PROCESSED TWEET IS: ['starv', ':-(']\n",
      "0\t1.00\tb'starv :-('\n",
      "THE TWEET IS: the internet is being a total bitch : (\n",
      "THE PROCESSED TWEET IS: ['internet', 'total', 'bitch']\n",
      "0\t1.00\tb'internet total bitch'\n",
      "THE TWEET IS: @_Jazdorothy cheer up :-(\n",
      "THE PROCESSED TWEET IS: ['cheer', ':-(']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1.00\tb'cheer :-('\n",
      "THE TWEET IS: worried :-(((((((((((\n",
      "THE PROCESSED TWEET IS: ['worri', ':-(']\n",
      "0\t1.00\tb'worri :-('\n",
      "THE TWEET IS: When Jessica calls and quits on power abs at 5:15 :-(\n",
      "THE PROCESSED TWEET IS: ['jessica', 'call', 'quit', 'power', 'ab', '5:15', ':-(']\n",
      "0\t1.00\tb'jessica call quit power ab 5:15 :-('\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth', 'httpst.co/wt4oxq5xcf']\n",
      "0\t1.00\tb'belov grandmoth httpst.co/wt4oxq5xcf'\n",
      "THE TWEET IS: when you don't have enough time to listen to all your artists' music :-(\n",
      "THE PROCESSED TWEET IS: ['enough', 'time', 'listen', 'artist', 'music', ':-(']\n",
      "0\t1.00\tb'enough time listen artist music :-('\n",
      "THE TWEET IS: @hyungwons_ tELL HIM TO PLS EAT MORE :-(((\n",
      "THE PROCESSED TWEET IS: ['tell', 'pl', 'eat', ':-(']\n",
      "0\t1.00\tb'tell pl eat :-('\n",
      "THE TWEET IS: @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
      "THE PROCESSED TWEET IS: ['that', 'life', 'get', 'call', 'peopl', 'havent', 'seen', '20', 'year', 'alway', 'favour']\n",
      "0\t1.00\tb'that life get call peopl havent seen 20 year alway favour'\n",
      "THE TWEET IS: @DeviousLiz awh :-( 💙 but you have to stay 💪\n",
      "THE PROCESSED TWEET IS: ['awh', ':-(', '💙', 'stay', '💪']\n",
      "0\t1.00\tb'awh :-(  stay '\n",
      "THE TWEET IS: @OXITS very disappointing to hear this, especially have just invested in one :-(\n",
      "THE PROCESSED TWEET IS: ['disappoint', 'hear', 'especi', 'invest', 'one', ':-(']\n",
      "0\t1.00\tb'disappoint hear especi invest one :-('\n",
      "THE TWEET IS: @Sakib_ovi so true :-(\n",
      "THE PROCESSED TWEET IS: ['true', ':-(']\n",
      "0\t1.00\tb'true :-('\n",
      "THE TWEET IS: thigh cramps :-(\n",
      "THE PROCESSED TWEET IS: ['thigh', 'cramp', ':-(']\n",
      "0\t1.00\tb'thigh cramp :-('\n",
      "THE TWEET IS: @pledis_17 I was not in the mood the whole day :-(\n",
      "THE PROCESSED TWEET IS: ['mood', 'whole', 'day', ':-(']\n",
      "0\t1.00\tb'mood whole day :-('\n",
      "THE TWEET IS: goTDAMN  :-( http://t.co/KkPDlQZ2F4\n",
      "THE PROCESSED TWEET IS: ['gotdamn', ':-(', 'httpt.co/kkpdlqz2f4']\n",
      "0\t1.00\tb'gotdamn :-( httpt.co/kkpdlqz2f4'\n",
      "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
      "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'httpt.co/ktknmhvwci', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
      "0\t1.00\tb'sr financi analyst expedia inc bellevu wa httpt.co/ktknmhvwci financ expediajob job job hire'\n",
      "THE TWEET IS: @ITVCentral #Midlands Yes thanks for the depressing weather forecast, where the word 'rain' was mentioned several times :-(\n",
      "THE PROCESSED TWEET IS: ['midland', 'ye', 'thank', 'depress', 'weather', 'forecast', 'word', 'rain', 'mention', 'sever', 'time', ':-(']\n",
      "0\t1.00\tb'midland ye thank depress weather forecast word rain mention sever time :-('\n",
      "THE TWEET IS: wonho reminds me of someone &gt;:( i just cant put my finger to it\n",
      "THE PROCESSED TWEET IS: ['wonho', 'remind', 'someon', '>:(', 'cant', 'put', 'finger']\n",
      "0\t1.00\tb'wonho remind someon >:( cant put finger'\n",
      "THE TWEET IS: :-( ugly af :-( https://t.co/bHIXkuNqPS\n",
      "THE PROCESSED TWEET IS: [':-(', 'ugli', 'af', ':-(', 'httpst.co/bhixkunqp']\n",
      "0\t1.00\tb':-( ugli af :-( httpst.co/bhixkunqp'\n",
      "THE TWEET IS: @eawoman As a Hull supporter I am expecting a misserable few weeks :-(\n",
      "THE PROCESSED TWEET IS: ['hull', 'support', 'expect', 'misser', 'week', ':-(']\n",
      "0\t1.00\tb'hull support expect misser week :-('\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(x_test, y_test):\n",
    "    y_hat = predict_tweet(x, gnb, freq_df)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', preprocess_tweet(x))\n",
    "        print('%d\\t%0.2f\\t%s' % (y, y_hat, ' '.join(preprocess_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Predict with your own tweet\n",
    "\n",
    "In this part you can predict the sentiment of your own tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# Test with your own tweet - feel free to modify `my_tweet`\n",
    "my_tweet = 'I am happy because I am learning :)'\n",
    "\n",
    "p = predict_tweet(my_tweet, gnb, freq_df)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on completing this assignment. See you next week!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "schema_names": [
    "NLPC1-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
