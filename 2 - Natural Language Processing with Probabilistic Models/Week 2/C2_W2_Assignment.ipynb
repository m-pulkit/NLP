{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Parts-of-Speech Tagging (POS)\n",
    "\n",
    "Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text.  Tagging is difficult because some words can represent more than one part of speech at different times. They are  **Ambiguous**. Let's look at the following example: \n",
    "\n",
    "- The whole team played **well**. [adverb]\n",
    "- You are doing **well** for yourself. [adjective]\n",
    "- **Well**, this assignment took me forever to complete. [interjection]\n",
    "- The **well** is dry. [noun]\n",
    "- Tears were beginning to **well** in her eyes. [verb]\n",
    "\n",
    "Distinguishing the parts-of-speech of a word in a sentence will help us better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, we will: \n",
    "\n",
    "- Learn how parts-of-speech tagging works\n",
    "- Compute the transition matrix A in a Hidden Markov Model\n",
    "- Compute the transition matrix B in a Hidden Markov Model\n",
    "- Compute the Viterbi algorithm \n",
    "- Compute the accuracy of our model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [0 Data Sources](#0)\n",
    "- [1 POS Tagging](#1)\n",
    "    - [1.1 Training](#1.1)\n",
    "        - [Exercise 01](#ex-01)\n",
    "    - [1.2 Testing](#1.2)\n",
    "        - [Exercise 02](#ex-02)\n",
    "- [2 Hidden Markov Models](#2)\n",
    "    - [2.1 Generating Matrices](#2.1)\n",
    "        - [Exercise 03](#ex-03)\n",
    "        - [Exercise 04](#ex-04)\n",
    "- [3 Viterbi Algorithm](#3)\n",
    "    - [3.1 Initialization](#3.1)\n",
    "        - [Exercise 05](#ex-05)\n",
    "    - [3.2 Viterbi Forward](#3.2)\n",
    "        - [Exercise 06](#ex-06)\n",
    "    - [3.3 Viterbi Backward](#3.3)\n",
    "        - [Exercise 07](#ex-07)\n",
    "- [4 Predicting on a data set](#4)\n",
    "    - [Exercise 08](#ex-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and loading in the data set \n",
    "#from utils_pos import get_word_tag, preprocess  \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "    \"\"\"\n",
    "    Assign tokens to unknown words\n",
    "    \"\"\"\n",
    "    punctuation = set(string.punctuation)    # Punctuation characters\n",
    "    \n",
    "    # Suffixes\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    # Loop the characters in the word, check if any is a digit\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return '--unk_digit--'\n",
    "    \n",
    "    # Loop the characters in the word, check if any is a punctuation character\n",
    "    elif any(char in punctuation for char in word):\n",
    "        return '--unk_punct--'\n",
    "    \n",
    "    # Loop the characters in the word, check if any is an upper case character\n",
    "    elif any(char.isupper() for char in word):\n",
    "        return '--unk_upper--'\n",
    "    \n",
    "    # Check if word ends with any noun suffix\n",
    "    elif any(word.endswith(ns) for ns in noun_suffix):\n",
    "        return '--unk_noun--'\n",
    "    \n",
    "    # Check if word ends with any verb suffix\n",
    "    elif any(word.endswith(vb) for vb in verb_suffix):\n",
    "        return '--unk_verb--'\n",
    "    \n",
    "    # Check if word ends with any adjective suffix\n",
    "    elif any(word.endswith(adj) for adj in adj_suffix):\n",
    "        return '--unk_adj--'\n",
    "    \n",
    "    # Check if word ends with any adverb suffix\n",
    "    elif any(word.endswith(adv) for adv in adv_suffix):\n",
    "        return '--unk_adv--'\n",
    "    \n",
    "    # If none of the previous criteria is met, return plain unknown\n",
    "    else:\n",
    "        return '--unk--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # If line is empty return placeholders for word and tag else Split line to separate word and tag\n",
    "    # Check if word is not in vocabulary\n",
    "    # Handle unknown word\n",
    "    words = line.split()\n",
    "    if not words:\n",
    "        return ('--n--', '--s--')\n",
    "    elif words[0] not in vocab:\n",
    "        return (assign_unk(words[0]), words[1])\n",
    "    else:\n",
    "        return tuple(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Part 0: Data Sources\n",
    "In this assignment, we will use two tagged data sets collected from the **Wall Street Journal (WSJ)**. \n",
    "\n",
    "[Here](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html) is an example 'tag-set' or Part of Speech designation describing the two or three letter tag and their meaning. \n",
    "- One data set (**WSJ-2_21.pos**) will be used for **training**.\n",
    "- The other (**WSJ-24.pos**) for **testing**. \n",
    "- The tagged training data has been preprocessed to form a vocabulary (**hmm_vocab.txt**). \n",
    "- The words in the vocabulary are words from the training set that were used two or more times. \n",
    "- The vocabulary is augmented with a set of 'unknown word tokens', described below. \n",
    "\n",
    "The training set will be used to create the emission, transmission and tag counts. \n",
    "\n",
    "The test set (WSJ-24.pos) is read in to create `y`. \n",
    "- This contains both the test text and the true tag. \n",
    "- The test set has also been preprocessed to remove the tags to form **test_words.txt**. \n",
    "- This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in **utils_pos.py**. \n",
    "- This forms the list `test_corpus`, the preprocessed text used to test our  POS taggers.\n",
    "\n",
    "A POS tagger will necessarily encounter words that are not in its datasets. \n",
    "- To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag. \n",
    "- For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'. \n",
    "- A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
    "\n",
    "\n",
    "<img src = \"DataSources1.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation note: \n",
    "\n",
    "- For python 3.6 and beyond, dictionaries retain the insertion order. \n",
    "- Furthermore, their hash-based lookup makes them suitable for rapid membership tests. \n",
    "    - If _di_ is a dictionary, `key in di` will return `True` if _di_ has a key _key_, else `False`. \n",
    "\n",
    "The dictionary `vocab` will utilize these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus in WSJ_02-21.pos\n",
    "with open('WSJ_02-21.pos', 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--', '.', '...', '0.01', '0.0108', '0.02', '0.03', '0.05', '0.1', '0.10', '0.12', '0.13', '0.15']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['yards', 'yardstick', 'year', 'year-ago', 'year-before', 'year-earlier', 'year-end', 'year-on-year', 'year-round', 'year-to-date', 'year-to-year', 'yearlong', 'yearly', 'years', 'yeast', 'yelled', 'yelling', 'yellow', 'yen', 'yes', 'yesterday', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yourself', 'youth', 'youthful', 'yuppie', 'yuppies', 'zero', 'zero-coupon', 'zeroing', 'zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary data hmm_vocab.txt, split by each line of text, and save the list\n",
    "with open('hmm_vocab.txt', 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "    \n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('!', 0),\n",
       " ('#', 1),\n",
       " ('$', 2),\n",
       " ('%', 3),\n",
       " ('&', 4),\n",
       " (\"'\", 5),\n",
       " (\"''\", 6),\n",
       " (\"'40s\", 7),\n",
       " (\"'60s\", 8),\n",
       " (\"'70s\", 9),\n",
       " (\"'80s\", 10),\n",
       " (\"'86\", 11),\n",
       " (\"'90s\", 12),\n",
       " (\"'N\", 13),\n",
       " (\"'S\", 14),\n",
       " (\"'d\", 15),\n",
       " (\"'em\", 16),\n",
       " (\"'ll\", 17),\n",
       " (\"'m\", 18),\n",
       " (\"'n'\", 19)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {word: i for i, word in enumerate(voc_l)}\n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "print('Vocabulary dictionary, key is the word, value is a unique integer')\n",
    "list(vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the test corpus WSJ_24.pos\n",
    "with open('WSJ_24.pos', 'r') as f:\n",
    "    test_tag_corpus = f.readlines()\n",
    "    \n",
    "print(test_tag_corpus[:10])\n",
    "len(test_tag_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk_noun--']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "test_corpus = [get_word_tag(word, vocab)[0] for word in test_tag_corpus]# if word != '\\n']\n",
    "print(test_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN', 'IN', 'JJ', 'NN']\n"
     ]
    }
   ],
   "source": [
    "#corpus true tags\n",
    "test_tags = [get_word_tag(word, vocab)[1] for word in test_tag_corpus]\n",
    "print(test_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34199, 34199, 23777)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus), len([w for w in test_corpus if w in vocab]), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%',\n",
       " 'gain',\n",
       " 'reported',\n",
       " 'Friday',\n",
       " 'in',\n",
       " 'the',\n",
       " 'producer',\n",
       " 'price',\n",
       " 'index',\n",
       " '.',\n",
       " '--n--',\n",
       " 'That',\n",
       " 'gain',\n",
       " 'was',\n",
       " 'being',\n",
       " 'cited',\n",
       " 'as',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'the']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# Part 1: Parts-of-speech tagging \n",
    "\n",
    "<a name='1.1'></a>\n",
    "## Part 1.1 - Training\n",
    "We will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art. \n",
    "\n",
    "In this section, We will find the words that are not ambiguous. \n",
    "- For example, the word `is` is a verb and it is not ambiguous. \n",
    "- In the `WSJ` corpus, $86$% of the token are unambiguous (meaning they have only one tag) \n",
    "- About $14\\%$ are ambiguous (meaning that they have more than one tag)\n",
    "\n",
    "<img src = \"pos.png\" style=\"width:400px;height:250px;\"/>\n",
    "\n",
    "Before we start predicting the tags of each word, we will need to compute a few dictionaries that will help generate the tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition counts\n",
    "- The first dictionary is the `transition_counts` dictionary which computes the number of times each tag happened next to another tag. \n",
    "\n",
    "This dictionary will be used to compute: \n",
    "$$P(t_i |t_{i-1}) \\tag{1}$$\n",
    "\n",
    "This is the probability of a tag at position $i$ given the tag at position $i-1$.\n",
    "\n",
    "In order to compute equation 1, lets create a `transition_counts` dictionary where \n",
    "- The keys are `(prev_tag, tag)`\n",
    "- The values are the number of times those two tags appeared in that order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission counts\n",
    "\n",
    "The second dictionary  we will compute is the `emission_counts` dictionary. This dictionary will be used to compute:\n",
    "\n",
    "$$P(w_i|t_i)\\tag{2}$$\n",
    "\n",
    "In other words, we will use it to compute the probability of a word given its tag. \n",
    "\n",
    "In order to compute equation 2, we will create an `emission_counts` dictionary where \n",
    "- The keys are `(tag, word)` \n",
    "- The values are the number of times that pair showed up in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag counts\n",
    "\n",
    "The last dictionary to compute is the `tag_counts` dictionary. \n",
    "- The key is the tag \n",
    "- The value is the number of times each tag appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-01'></a>\n",
    "### Exercise 01\n",
    "\n",
    "Write a program that takes in the `training_corpus` and returns the three dictionaries mentioned above `transition_counts`, `emission_counts`, and `tag_counts`. \n",
    "- `emission_counts`: maps (tag, word) to the number of times it happened. \n",
    "- `transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. \n",
    "- `tag_counts`: maps (tag) to the number of times it has occured. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\t.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        training_corpus: a corpus where each line has a word followed by its tag.\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output: \n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        transition_counts: a dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
    "        tag_counts: a dictionary where the keys are the tags and the values are the counts\n",
    "    \"\"\"\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    prev_tag = '--s--'\n",
    "    \n",
    "    for line in training_corpus:\n",
    "        word, tag = get_word_tag(line, vocab)\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "['IN', 'DT', 'NNP', 'CD', 'NN', '``', \"''\", 'POS', '(', 'VBN', 'NNS', 'VBP', ',', 'CC', ')', 'VBD', 'RB', 'TO', '.', '--s--', 'VBZ', 'NNPS', 'PRP', 'PRP$', 'VB', 'JJ', 'MD', 'VBG', 'RBR', ':', 'WP', 'WDT', 'JJR', 'PDT', 'RBS', 'WRB', 'JJS', '$', 'RP', 'FW', 'EX', 'SYM', '#', 'LS', 'UH', 'WP$']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "print(f\"Number of POS tags (number of 'states'): {len(tag_counts)}\")\n",
    "print([tag for tag in tag_counts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this assignment. \n",
    "\n",
    "- \"NN\" is noun, singular, \n",
    "- 'NNS' is noun, plural. \n",
    "- In addition, there are helpful tags like '--s--' which indicate a start of a sentence.\n",
    "- A more complete description is at [Penn Treebank II tag set](https://www.clips.uantwerpen.be/pages/mbsp-tags). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples:  ((('--s--', 'IN'), 5050), (('IN', 'DT'), 32364), (('DT', 'NNP'), 9044))\n",
      "emission examples:  ((('IN', 'In'), 1735), (('DT', 'an'), 3142), (('NNP', 'Oct.'), 317))\n",
      "\n",
      "ambigous word examples: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \", tuple(transition_counts.items())[:3])\n",
    "print(\"emission examples: \", tuple(emission_counts.items())[:3])\n",
    "print(\"\\nambigous word examples: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### Part 1.2 - Testing\n",
    "\n",
    "Now we will test the accuracy of our parts-of-speech tagger using the `emission_counts` dictionary. \n",
    "- Given the preprocessed test corpus, we will assign a parts-of-speech tag to every word in that corpus. \n",
    "- Using the original tagged test corpus, we will then compute what percent of the tags we got correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-02'></a>\n",
    "### Exercise 02\n",
    "\n",
    "Implement `predict_pos` that computes the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(test_corpus, y, emission_counts, vocab, states):\n",
    "    '''\n",
    "    Input: \n",
    "        test_corpus: a preprocessed version of 'y'. A list with the 'word' component of the tuples.\n",
    "        y: a corpus composed of a list of tuples where each tuple consists of (word, POS)\n",
    "        emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "        states: a sorted list of all possible tags for this assignment\n",
    "    Output: \n",
    "        accuracy: Number of times we classified a word correctly\n",
    "    '''\n",
    "    \n",
    "    count = 0\n",
    "    i = 0\n",
    "    \n",
    "    for word in test_corpus:\n",
    "        if word not in vocab:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        candidates = {tag: emission_counts[(tag,word)] for tag in states \n",
    "                      if (tag,word) in emission_counts}\n",
    "        pred_tag = [tag for tag in candidates if candidates[tag] == max(candidates.values())][0]\n",
    "        \n",
    "        # print(word, candidates, pred_tag, y[i][1])\n",
    "        if pred_tag == y[i][1]:\n",
    "            count += 1\n",
    "        i += 1\n",
    "        \n",
    "    return round(count/i, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.9307\n"
     ]
    }
   ],
   "source": [
    "states = sorted(tag_counts.keys())\n",
    "print(\"Accuracy of prediction using predict_pos is\", \n",
    "      predict_pos(test_corpus, [get_word_tag(line, vocab) for line in test_tag_corpus], \n",
    "                  emission_counts, vocab, states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "86.6% is really good for this warm up exercise. With hidden markov models, we should be able to get **95% accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Hidden Markov Models for POS\n",
    "\n",
    "Now we will build something more context specific. Concretely, we will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder\n",
    "- The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques. \n",
    "- In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. \n",
    "- By completing this part of the assignment we will get a 95% accuracy on the same dataset we used in Part 1.\n",
    "\n",
    "The Markov Model contains a number of states and the probability of transition between those states. \n",
    "- In this case, the states are the parts-of-speech. \n",
    "- A Markov Model utilizes a transition matrix, `A`. \n",
    "- A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when we are in a particular state. \n",
    "- In this case, the emissions are the words in the corpus\n",
    "- The state, which is hidden, is the POS tag of that word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "## Part 2.1 Generating Matrices\n",
    "\n",
    "### Creating the 'A' transition probabilities matrix\n",
    "Now that we have `emission_counts`, `transition_counts`, and `tag_counts`, we will start implementing the Hidden Markov Model. \n",
    "\n",
    "This will allow us to quickly construct the \n",
    "- `A` transition probabilities matrix.\n",
    "- and the `B` emission probabilities matrix. \n",
    "\n",
    "We will also use some smoothing when computing these matrices. \n",
    "\n",
    "Here is an example of what the `A` transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n",
    "\n",
    "\n",
    "|**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...\n",
    "| --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----\n",
    "|**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...\n",
    "|**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...\n",
    "|**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...\n",
    "|**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...\n",
    "|**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...\n",
    "| ... |...| ...          | ...          | ...          | ...      | ...        | ...\n",
    "\n",
    "Note that the matrix above was computed with smoothing. \n",
    "\n",
    "Each cell gives us the probability to go from one part of speech to another. \n",
    "- In other words, there is a 4.47e-8 chance of going from parts-of-speech `TO` to `RP`. \n",
    "- The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n",
    "\n",
    "The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-03'></a>\n",
    "### Exercise 03\n",
    "\n",
    "Implement the `create_transition_matrix` below for all tags. The task is to output a matrix that computes equation 3 for each cell in matrix `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    N = len(tag_counts)\n",
    "    A = np.zeros((N, N))\n",
    "    \n",
    "    for i,tag_i in enumerate(sorted(tag_counts.keys())):\n",
    "        for j,tag_j in enumerate(sorted(tag_counts.keys())):\n",
    "            A[i, j] = (alpha + transition_counts[(tag_i, tag_j)]) / (tag_counts[tag_i] + alpha*N)\n",
    "            \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0:  7.039972966503809e-06\n",
      "A at row 3, col 1:  0.16910191896905374\n",
      "Viewing a subset of transition matrix A\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RBS</th>\n",
       "      <th>RP</th>\n",
       "      <th>SYM</th>\n",
       "      <th>TO</th>\n",
       "      <th>UH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>2.217069e-06</td>\n",
       "      <td>2.217069e-06</td>\n",
       "      <td>2.217069e-06</td>\n",
       "      <td>0.008870</td>\n",
       "      <td>2.217069e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>7.516775e-04</td>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>0.051089</td>\n",
       "      <td>3.756509e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SYM</th>\n",
       "      <td>1.722772e-05</td>\n",
       "      <td>1.722772e-05</td>\n",
       "      <td>1.722772e-05</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1.722772e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>4.477336e-05</td>\n",
       "      <td>4.472863e-08</td>\n",
       "      <td>4.472863e-08</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>4.477336e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UH</th>\n",
       "      <td>1.030439e-05</td>\n",
       "      <td>1.030439e-05</td>\n",
       "      <td>1.030439e-05</td>\n",
       "      <td>0.061837</td>\n",
       "      <td>3.092348e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              RBS            RP           SYM        TO            UH\n",
       "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
       "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
       "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
       "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
       "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = create_transition_matrix(0.001, tag_counts, transition_counts)\n",
    "print(\"A at row 0, col 0: \", A[0,0])\n",
    "print(\"A at row 3, col 1: \", A[3,1])\n",
    "\n",
    "print(\"Viewing a subset of transition matrix A\")\n",
    "pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 'B' emission probabilities matrix\n",
    "\n",
    "Now We will create the `B` transition matrix which computes the emission probability. \n",
    "\n",
    "We will use smoothing as defined below: \n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. \n",
    "\n",
    "The matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. \n",
    "\n",
    "Here is an example of the matrix, only a subset of tags and words are shown: \n",
    "<p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>\n",
    "\n",
    "|**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|\n",
    "|----|----|--------------|--------------|--------------|--------------|-------------|----|\n",
    "|**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|\n",
    "|**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|\n",
    "|**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|\n",
    "|**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|\n",
    "|**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|\n",
    "|**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|\n",
    "| ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-04'></a>\n",
    "### Exercise 04\n",
    "Implement the `create_emission_matrix` below that computes the `B` emission probabilities matrix. The task is to output a matrix that computes equation 4 for each cell in matrix `B`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    N = len(tag_counts)\n",
    "    B = np.zeros((N, len(vocab)))\n",
    "    \n",
    "    for i,tag in enumerate(sorted(tag_counts.keys())):\n",
    "        for j,word in enumerate(vocab):\n",
    "            B[i, j] = (alpha + emission_counts[(tag, word)]) / (tag_counts[tag] + alpha*N)\n",
    "            \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B[rows, [vocab[c] for c in cols]].shape\n",
    "#print(np.ix_([states.index(t) for t in rows],[vocab[c] for c in cols]))\n",
    "#[np.reshape([states.index(t) for t in rows], (-1,1)), [vocab[c] for c in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B at row 0, col 0:  7.039972966503809e-06\n",
      "B at row 3, col 1:  7.320397702566385e-07\n",
      "Viewing a subset of emission matrix B\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>725</th>\n",
       "      <th>adroitly</th>\n",
       "      <th>engineers</th>\n",
       "      <th>promoted</th>\n",
       "      <th>synergy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>8.206618e-05</td>\n",
       "      <td>2.734628e-08</td>\n",
       "      <td>2.734628e-08</td>\n",
       "      <td>2.734628e-08</td>\n",
       "      <td>2.734628e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>7.522471e-09</td>\n",
       "      <td>7.522471e-09</td>\n",
       "      <td>7.522471e-09</td>\n",
       "      <td>7.522471e-09</td>\n",
       "      <td>2.257493e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>1.670675e-08</td>\n",
       "      <td>1.670675e-08</td>\n",
       "      <td>4.678057e-04</td>\n",
       "      <td>1.670675e-08</td>\n",
       "      <td>1.670675e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>3.782428e-08</td>\n",
       "      <td>3.782428e-08</td>\n",
       "      <td>3.782428e-08</td>\n",
       "      <td>3.782428e-08</td>\n",
       "      <td>3.782428e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>3.228926e-08</td>\n",
       "      <td>6.461082e-05</td>\n",
       "      <td>3.228926e-08</td>\n",
       "      <td>3.228926e-08</td>\n",
       "      <td>3.228926e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RP</th>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>3.756509e-07</td>\n",
       "      <td>3.756509e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              725      adroitly     engineers      promoted       synergy\n",
       "CD   8.206618e-05  2.734628e-08  2.734628e-08  2.734628e-08  2.734628e-08\n",
       "NN   7.522471e-09  7.522471e-09  7.522471e-09  7.522471e-09  2.257493e-05\n",
       "NNS  1.670675e-08  1.670675e-08  4.678057e-04  1.670675e-08  1.670675e-08\n",
       "VB   3.782428e-08  3.782428e-08  3.782428e-08  3.782428e-08  3.782428e-08\n",
       "RB   3.228926e-08  6.461082e-05  3.228926e-08  3.228926e-08  3.228926e-08\n",
       "RP   3.756509e-07  3.756509e-07  3.756509e-07  3.756509e-07  3.756509e-07"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(0.001, tag_counts, emission_counts, vocab)\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "print(\"B at row 0, col 0: \", B[0,0])\n",
    "print(\"B at row 3, col 1: \", B[3,1])\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe and get their indices\n",
    "rows = ['CD', 'NN', 'NNS', 'VB', 'RB', 'RP']\n",
    "\n",
    "# Select words to display\n",
    "cols = ['725', 'adroitly', 'engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "print(\"Viewing a subset of emission matrix B\")\n",
    "pd.DataFrame(B[[states.index(t) for t in rows]][:, [vocab[c] for c in cols]], index=rows, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# Part 3: Viterbi Algorithm and Dynamic Programming\n",
    "\n",
    "In this part of the assignment we will implement the Viterbi algorithm which makes use of dynamic programming. Specifically, we will use two matrices, `A` and `B` to compute the Viterbi algorithm. We have decomposed this process into three main steps.\n",
    "\n",
    "* **Initialization** - In this part we initialize the `best_paths` and `best_probabilities` matrices that we will be populating in `feed_forward`.\n",
    "* **Feed forward** - At each step, we calculate the probability of each path happening and the best paths up to that point. \n",
    "* **Feed backward**: This allows us to find the best path with the highest probabilities. \n",
    "\n",
    "<a name='3.1'></a>\n",
    "## Part 3.1:  Initialization \n",
    "\n",
    "we will start by initializing two matrices of the same dimension. \n",
    "\n",
    "- best_probs: Each cell contains the probability of going from one POS tag to a word in the corpus.\n",
    "\n",
    "- best_paths: A matrix that helps trace through the best possible path in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-05'></a>\n",
    "### Exercise 05\n",
    "Task: To initialize the `best_probs` and the `best_paths` matrix. \n",
    "\n",
    "Both matrices will be initialized to zero except for column zero of `best_probs`.  \n",
    "- Column zero of `best_probs` is initialized with the assumption that the first word of the corpus was preceded by a start token (\"--s--\"). \n",
    "- This allows us to reference the **A** matrix for the transition probability\n",
    "\n",
    "Here is how to initialize column 0 of `best_probs`:\n",
    "- The probability of the best path going from the start index to a given POS tag indexed by integer $i$ is denoted by $\\textrm{best_probs}[s_{idx}, i]$.\n",
    "- This is estimated as the probability that the start tag transitions to the POS denoted by index $i$: $\\mathbf{A}[s_{idx}, i]$ AND that the POS tag denoted by $i$ emits the first word of the given corpus, which is $\\mathbf{B}[i, vocab[corpus[0]]]$.\n",
    "- Note that vocab[corpus[0]] refers to the first word of the corpus (the word at position 0 of the corpus). \n",
    "- **vocab** is a dictionary that returns the unique integer that refers to that particular word.\n",
    "\n",
    "Conceptually, it looks like this:\n",
    "$\\textrm{best_probs}[s_{idx}, i] = \\mathbf{A}[s_{idx}, i] \\times \\mathbf{B}[i, corpus[0] ]$\n",
    "\n",
    "\n",
    "In order to avoid multiplying and storing small values on the computer, we'll take the log of the product, which becomes the sum of two logs:\n",
    "\n",
    "$best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n",
    "\n",
    "Also, to avoid taking the log of 0 (which is defined as negative infinity), the code itself will just set $best\\_probs[i,0] = float('-inf')$ when $A[s_{idx}, i] == 0$\n",
    "\n",
    "\n",
    "So the implementation to initialize $best\\_probs$ looks like this:\n",
    "\n",
    "$ if A[s_{idx}, i] <> 0 : best\\_probs[i,0] = log(A[s_{idx}, i]) + log(B[i, vocab[corpus[0]]$\n",
    "\n",
    "$ if A[s_{idx}, i] == 0 : best\\_probs[i,0] = float('-inf')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows the initialization assuming the corpus starts with the phrase \"Loss tracks upward\".\n",
    "\n",
    "<img src = \"Initialize4.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent infinity and negative infinity like this:\n",
    "\n",
    "```CPP\n",
    "float('inf')\n",
    "float('-inf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "    best_probs = np.zeros((len(tag_counts), len(corpus)))\n",
    "    best_paths = np.zeros((len(tag_counts), len(corpus)))\n",
    "    sidx = states.index(\"--s--\")\n",
    "    \n",
    "    for i in range(len(tag_counts)):\n",
    "        if A[sidx, i] != 0:\n",
    "            best_probs[i, 0] = math.log(A[sidx, i]) + math.log(B[i, vocab[corpus[0]]])\n",
    "        else:\n",
    "            best_probs[i, 0] = float('-inf')\n",
    "        \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]:  -22.46\n",
      "best_paths[2,3]:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, test_corpus, vocab)\n",
    "print(\"best_probs[0,0]: \", round(best_probs[0,0], 2))\n",
    "print(\"best_paths[2,3]: \", best_paths[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "## Part 3.2 Viterbi Forward\n",
    "\n",
    "In this part of the assignment, we will implement the `viterbi_forward` segment. In other words, we will populate the `best_probs` and `best_paths` matrices.\n",
    "- Walk forward through the corpus.\n",
    "- For each word, compute a probability for each possible tag. \n",
    "- Unlike the previous algorithm `predict_pos` (the 'warm-up' exercise), this will include the path up to that (word,tag) combination. \n",
    "\n",
    "Here is an example with a three-word corpus \"Loss tracks upward\":\n",
    "- Note, in this example, only a subset of states (POS tags) are shown in the diagram below, for easier reading. \n",
    "- In the diagram below, the first word \"Loss\" is already initialized. \n",
    "- The algorithm will compute a probability for each of the potential tags in the second and future words. \n",
    "\n",
    "Compute the probability that the tag of the second work ('tracks') is a verb, 3rd person singular present (VBZ).  \n",
    "- In the `best_probs` matrix, go to the column of the second word ('tracks'), and row 40 (VBZ), this cell is highlighted in light orange in the diagram below.\n",
    "- Examine each of the paths from the tags of the first word ('Loss') and choose the most likely path.  \n",
    "- An example of the calculation for **one** of those paths is the path from ('Loss', NN) to ('tracks', VBZ).\n",
    "- The log of the probability of the path up to and including the first word 'Loss' having POS tag NN is $-14.32$.  The `best_probs` matrix contains this value -14.32 in the column for 'Loss' and row for 'NN'.\n",
    "- Find the probability that NN transitions to VBZ.  To find this probability, go to the `A` transition matrix, and go to the row for 'NN' and the column for 'VBZ'.  The value is $4.37e-02$, which is circled in the diagram, so add $-14.32 + log(4.37e-02)$. \n",
    "- Find the log of the probability that the tag VBS would 'emit' the word 'tracks'.  To find this, look at the 'B' emission matrix in row 'VBZ' and the column for the word 'tracks'.  The value $4.61e-04$ is circled in the diagram below.  So add $-14.32 + log(4.37e-02) + log(4.61e-04)$.\n",
    "- The sum of $-14.32 + log(4.37e-02) + log(4.61e-04)$ is $-25.13$. Store $-25.13$ in the `best_probs` matrix at row 'VBZ' and column 'tracks' (as seen in the cell that is highlighted in light orange in the diagram).\n",
    "- All other paths in best_probs are calculated.  Notice that $-25.13$ is greater than all of the other values in column 'tracks' of matrix `best_probs`, and so the most likely path to 'VBZ' is from 'NN'.  'NN' is in row 20 of the `best_probs` matrix, so $20$ is the most likely path.\n",
    "- Store the most likely path $20$ in the `best_paths` table.  This is highlighted in light orange in the diagram below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the formula to compute the probability and path for the $i^{th}$ word in the $corpus$, the prior word $i-1$ in the corpus, current POS tag $j$, and previous POS tag $k$ is:\n",
    "\n",
    "$\\mathrm{prob} = \\mathbf{best\\_prob}_{k, i-1} + \\mathrm{log}(\\mathbf{A}_{k, j}) + \\mathrm{log}(\\mathbf{B}_{j, vocab(corpus_{i})})$\n",
    "\n",
    "where $corpus_{i}$ is the word in the corpus at index $i$, and $vocab$ is the dictionary that gets the unique integer that represents a given word.\n",
    "\n",
    "$\\mathrm{path} = k$\n",
    "\n",
    "where $k$ is the integer representing the previous POS tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-06'></a>\n",
    "\n",
    "### Exercise 06\n",
    "\n",
    "Lets implement the `viterbi_forward` algorithm and store the best_path and best_prob for every possible tag for each word in the matrices `best_probs` and `best_tags`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Forward4.PNG\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46, 23777), 34199, (46, 34199))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape, len(test_corpus), best_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    prob = np.zeros(best_probs.shape[0])\n",
    "    \n",
    "    for i in range(1, best_probs.shape[1]):\n",
    "        for j in range(best_probs.shape[0]):\n",
    "            for k in range(best_probs.shape[0]):\n",
    "                prob[k] = best_probs[k, i-1] + math.log(A[k,j]) + math.log(B[j,vocab[test_corpus[i]]])\n",
    "            \n",
    "            k = prob.argmax()\n",
    "            best_paths[j,i] = k\n",
    "            best_probs[j,i] = prob[k] \n",
    "            # if (i % 5000 == 0) & (j % 10 == 0):\n",
    "            #    print(k, B[j, vocab[test_corpus[i]]], prob)\n",
    "            \n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: \", i)\n",
    "    \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46, 23777), (46, 46))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the `viterbi_forward` function to fill in the `best_probs` and `best_paths` matrices.\n",
    "\n",
    "**Note** that this will take a few minutes to run.  There are about 30,000 words to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:  5000\n",
      "Words processed:  10000\n",
      "Words processed:  15000\n",
      "Words processed:  20000\n",
      "Words processed:  25000\n",
      "Words processed:  30000\n"
     ]
    }
   ],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.63 and best_path = 11.00\n",
      "best_probs[0,4]: -49.40 and best_path = 20.00\n"
     ]
    }
   ],
   "source": [
    "# Testing this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.2f} and best_path = {best_paths[0,1]:.2f}\")\n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.2f} and best_path = {best_paths[0,4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "## Part 3.3 Viterbi backward\n",
    "\n",
    "Now we will implement the Viterbi backward algorithm.\n",
    "- The Viterbi backward algorithm gets the predictions of the POS tags for each word in the corpus using the `best_paths` and the `best_probs` matrices.\n",
    "\n",
    "The example below shows how to walk backwards through the best_paths matrix to get the POS tags of each word in the corpus. Recall that this example corpus has three words: \"Loss tracks upward\".\n",
    "\n",
    "POS tag for 'upward' is `RB`\n",
    "- Select the the most likely POS tag for the last word in the corpus, 'upward' in the `best_prob` table.\n",
    "- Look for the row in the column for 'upward' that has the largest probability.\n",
    "- Notice that in row 28 of `best_probs`, the estimated probability is -34.99, which is larger than the other values in the column.  So the most likely POS tag for 'upward' is `RB` an adverb, at row 28 of `best_prob`. \n",
    "- The variable `z` is an array that stores the unique integer ID of the predicted POS tags for each word in the corpus.  In array z, at position 2, store the value 28 to indicate that the word 'upward' (at index 2 in the corpus), most likely has the POS tag associated with unique ID 28 (which is `RB`).\n",
    "- The variable `pred` contains the POS tags in string form.  So `pred` at index 2 stores the string `RB`.\n",
    "\n",
    "\n",
    "POS tag for 'tracks' is `VBZ`\n",
    "- The next step is to go backward one word in the corpus ('tracks').  Since the most likely POS tag for 'upward' is `RB`, which is uniquely identified by integer ID 28, go to the `best_paths` matrix in column 2, row 28.  The value stored in `best_paths`, column 2, row 28 indicates the unique ID of the POS tag of the previous word.  In this case, the value stored here is 40, which is the unique ID for POS tag `VBZ` (verb, 3rd person singular present).\n",
    "- So the previous word at index 1 of the corpus ('tracks'), most likely has the POS tag with unique ID 40, which is `VBZ`.\n",
    "- In array `z`, store the value 40 at position 1, and for array `pred`, store the string `VBZ` to indicate that the word 'tracks' most likely has POS tag `VBZ`.\n",
    "\n",
    "POS tag for 'Loss' is `NN`\n",
    "- In `best_paths` at column 1, the unique ID stored at row 40 is 20.  20 is the unique ID for POS tag `NN`.\n",
    "- In array `z` at position 0, store 20.  In array `pred` at position 0, store `NN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Backwards5.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-07'></a>\n",
    "### Exercise 07\n",
    "Lets implement the `viterbi_backward` algorithm, which returns a list of predicted POS tags for each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[10, 0, 0], \n",
    "          [330., 0., 50.], \n",
    "          [-30., 0., -40.], \n",
    "          [0., 5., 54.], \n",
    "          [2.2, 6, -4], \n",
    "          [2,-50, 0]]).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path lists (both indices and tag names). \n",
    "    \n",
    "    '''\n",
    "    z = np.zeros(len(test_corpus), dtype=int)  # best_probs.argmax(0)\n",
    "    pred = np.zeros(len(test_corpus), dtype='U5')  # [states[s] for s in z]\n",
    "    \n",
    "    z[-1] = best_probs[:, -1].argmax()\n",
    "    pred[-1] = states[z[-1]]\n",
    "    \n",
    "    for i in range(len(test_corpus)-1, 0, -1):\n",
    "        z[i-1] = best_paths[z[i], i]\n",
    "        pred[i-1] = states[z[i-1]]\n",
    "        \n",
    "    return z, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is:\n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB' 'PRP' 'RB' 'IN' 'PRP' '.']\n",
      "\n",
      "The prediction for pred[0:8] is:\n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from'] \n",
      " ['DT' 'NN' 'POS' 'NN' 'MD' 'VB' 'VBN' 'IN']\n"
     ]
    }
   ],
   "source": [
    "# Testing the function for the last few words of the corpus and their states to aid in debug.\n",
    "z, pred = viterbi_backward(best_probs, best_paths, test_corpus, states)\n",
    "print(\"The prediction for pred[-7:m-1] is:\\n\", test_corpus[-7:-1], \"\\n\", pred[-7:-1])\n",
    "print(\"\\nThe prediction for pred[0:8] is:\\n\", test_corpus[0:8], \"\\n\", pred[0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# Part 4: Predicting on a data set\n",
    "\n",
    "Lets compute the accuracy of our prediction by comparing it with the true `y` labels. \n",
    "- `pred` is a list of predicted POS tags corresponding to the words of the `test_corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third word is: temperature\n",
      "The prediction is:  NN\n"
     ]
    }
   ],
   "source": [
    "print(f\"The third word is: {test_corpus[3]}\")\n",
    "print(\"The prediction is: \", pred[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-08'></a>\n",
    "### Exercise 08\n",
    "\n",
    "Lets compute the accuracy of the viterbi algorithm's POS tag predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the Viterbi algorithm is {:.2f}\".format(np.mean(pred==test_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen we were able to classify the parts-of-speech with 96% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points and overview\n",
    "\n",
    "In this assignment we learned about parts-of-speech tagging. \n",
    "- In this assignment, we predicted POS tags by walking forward through a corpus and knowing the previous word.\n",
    "- There are other implementations that use bidirectional POS tagging.\n",
    "- Bidirectional POS tagging requires knowing the previous word and the next word in the corpus when predicting the current word's POS tag.\n",
    "- Bidirectional POS tagging would tell more about the POS instead of just knowing the previous word. \n",
    "- Since we have learned to implement the unidirectional approach, we have the foundation to implement other POS taggers used in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [\"Speech and Language Processing\", Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- We would like to thank Melanie Tosik for her help and inspiration"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
